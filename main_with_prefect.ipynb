{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1ddf80",
   "metadata": {},
   "source": [
    "# Main Program\n",
    "\n",
    "This notebook orchestrates a data pipeline to load datasets, engineer features, train a model to predict tips, and merge predictions with a template. The pipeline is managed using Prefect for workflow orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046876ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:28:58.649 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | prefect - Starting temporary server on <span style=\"color: #0000ff; text-decoration-color: #0000ff\">http://127.0.0.1:8018</span>\n",
       "See <span style=\"color: #0000ff; text-decoration-color: #0000ff\">https://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server</span> for more information on running a dedicated Prefect server.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:28:58.649 | \u001b[36mINFO\u001b[0m    | prefect - Starting temporary server on \u001b[94mhttp://127.0.0.1:8018\u001b[0m\n",
       "See \u001b[94mhttps://docs.prefect.io/3.0/manage/self-host#self-host-a-prefect-server\u001b[0m for more information on running a dedicated Prefect server.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:29:06.611 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'rose-raptor'</span> - Beginning flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'rose-raptor'</span> for flow<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> 'Tip-Prediction-Pipeline'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:29:06.611 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'rose-raptor'\u001b[0m - Beginning flow run\u001b[35m 'rose-raptor'\u001b[0m for flow\u001b[1;35m 'Tip-Prediction-Pipeline'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:29:31.344 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'load_data-a81' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:29:31.344 | \u001b[36mINFO\u001b[0m    | Task run 'load_data-a81' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DABI 4\\DABI-2\\features.py:399: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged['order_count'] = merged.groupby('department').cumcount().astype('int32')\n",
      "d:\\DABI 4\\DABI-2\\features.py:400: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged['tip_cumsum_before'] = merged.groupby('department')['tip'].cumsum() - merged['tip']\n",
      "d:\\DABI 4\\DABI-2\\features.py:432: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged['order_count'] = merged.groupby('aisle').cumcount().astype('int32')\n",
      "d:\\DABI 4\\DABI-2\\features.py:433: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  merged['tip_cumsum_before'] = merged.groupby('aisle')['tip'].cumsum() - merged['tip']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:32:09.807 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'engineer_features-376' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:32:09.807 | \u001b[36mINFO\u001b[0m    | Task run 'engineer_features-376' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:32:37.613 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'save_features-d5d' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:32:37.613 | \u001b[36mINFO\u001b[0m    | Task run 'save_features-d5d' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:32:39.308 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'split_data-8bb' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:32:39.308 | \u001b[36mINFO\u001b[0m    | Task run 'split_data-8bb' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:36:51.388 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'train_model-90b' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:36:51.388 | \u001b[36mINFO\u001b[0m    | Task run 'train_model-90b' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:39:53.220 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'predict_tips-fbc' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:39:53.220 | \u001b[36mINFO\u001b[0m    | Task run 'predict_tips-fbc' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:39:55.305 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'merge_with_template-ee0' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:39:55.305 | \u001b[36mINFO\u001b[0m    | Task run 'merge_with_template-ee0' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:39:56.152 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Task run 'save_predictions-abb' - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:39:56.152 | \u001b[36mINFO\u001b[0m    | Task run 'save_predictions-abb' - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">15:40:03.348 | <span style=\"color: #008080; text-decoration-color: #008080\">INFO</span>    | Flow run<span style=\"color: #800080; text-decoration-color: #800080\"> 'rose-raptor'</span> - Finished in state <span style=\"color: #008000; text-decoration-color: #008000\">Completed</span>()\n",
       "</pre>\n"
      ],
      "text/plain": [
       "15:40:03.348 | \u001b[36mINFO\u001b[0m    | Flow run\u001b[35m 'rose-raptor'\u001b[0m - Finished in state \u001b[32mCompleted\u001b[0m()\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import features\n",
    "from prefect import task, flow\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec8fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@task\n",
    "def load_data(orders_path: str, products_path: str, tips_path: str) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load and preprocess the datasets.\n",
    "\n",
    "    Args:\n",
    "        orders_path (str): Path to orders dataset.\n",
    "        products_path (str): Path to order products dataset.\n",
    "        tips_path (str): Path to tips dataset.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (orders, order_products_denormalized, tips_public) DataFrames.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If any file path is invalid.\n",
    "    \"\"\"\n",
    "    logger.info(\"Loading datasets...\")\n",
    "    try:\n",
    "        orders = pd.read_parquet(orders_path)\n",
    "        order_products_denormalized = pd.read_csv(products_path, dtype={'order_id': 'int64'})\n",
    "        tips_public = pd.read_csv(tips_path, dtype={'order_id': 'int64'}).drop(columns=[\"Unnamed: 0\"], errors='ignore')\n",
    "\n",
    "        # Optimize memory usage\n",
    "        order_products_denormalized['department'] = order_products_denormalized['department'].astype('category')\n",
    "        order_products_denormalized['aisle'] = order_products_denormalized['aisle'].astype('category')\n",
    "\n",
    "        # Ensure consistent data types\n",
    "        orders['order_id'] = orders['order_id'].astype('int64')\n",
    "        orders['user_id'] = orders['user_id'].astype('int64')\n",
    "        order_products_denormalized['product_id'] = order_products_denormalized['product_id'].astype('int64')\n",
    "        tips_public['order_id'] = tips_public['order_id'].astype('int64')\n",
    "\n",
    "        logger.info(f\"Datasets loaded: orders={orders.shape}, products={order_products_denormalized.shape}, tips={tips_public.shape}\")\n",
    "        return orders, order_products_denormalized, tips_public\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"File not found: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def engineer_features(orders: pd.DataFrame, order_products_denormalized: pd.DataFrame, tips_public: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate engineered features using the features module.\n",
    "\n",
    "    Args:\n",
    "        orders (pd.DataFrame): Orders dataset.\n",
    "        order_products_denormalized (pd.DataFrame): Order products dataset.\n",
    "        tips_public (pd.DataFrame): Tips dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with all engineered features.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If feature engineering fails.\n",
    "    \"\"\"\n",
    "    logger.info(\"Engineering features...\")\n",
    "    try:\n",
    "        all_features_df = features.combine_all_features(orders, order_products_denormalized, tips_public)\n",
    "        logger.info(f\"Features engineered: {all_features_df.shape}, columns={all_features_df.columns.tolist()}\")\n",
    "        return all_features_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Feature engineering failed: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def save_features(all_features_df: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Save the feature DataFrame to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        all_features_df (pd.DataFrame): DataFrame with features.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "\n",
    "    Raises:\n",
    "        IOError: If saving fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Saving features to {output_path}...\")\n",
    "    try:\n",
    "        all_features_df.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Features saved successfully to {output_path}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Failed to save features: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def split_data(all_features_df: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data into training (non-NaN tips) and prediction (NaN tips) sets.\n",
    "\n",
    "    Args:\n",
    "        all_features_df (pd.DataFrame): DataFrame with all features.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (train_df, predict_df) DataFrames.\n",
    "    \"\"\"\n",
    "    logger.info(\"Splitting data...\")\n",
    "    train_df = all_features_df[~all_features_df['tip'].isna()]\n",
    "    predict_df = all_features_df[all_features_df['tip'].isna()]\n",
    "    logger.info(f\"Train data: {train_df.shape}, Predict data: {predict_df.shape}\")\n",
    "    return train_df, predict_df\n",
    "\n",
    "@task\n",
    "def train_model(train_df: pd.DataFrame, feature_columns: list[str]) -> tuple[RandomForestClassifier, float, str]:\n",
    "    \"\"\"Train a Random Forest Classifier on the training data.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): Training DataFrame.\n",
    "        feature_columns (list[str]): List of feature column names.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (trained model, accuracy, classification report).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns are missing or data is empty.\n",
    "    \"\"\"\n",
    "    logger.info(\"Training model...\")\n",
    "    try:\n",
    "        if train_df.empty:\n",
    "            raise ValueError(\"Training DataFrame is empty.\")\n",
    "        missing_cols = [col for col in feature_columns if col not in train_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns: {missing_cols}\")\n",
    "\n",
    "        X = train_df[feature_columns]\n",
    "        y = train_df['tip'].astype('int')  # Binary classification\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        report = classification_report(y_test, y_pred)\n",
    "\n",
    "        logger.info(f\"Model trained. Accuracy: {accuracy:.4f}\")\n",
    "        logger.info(f\"Classification Report:\\n{report}\")\n",
    "        return model, accuracy, report\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Model training failed: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def predict_tips(model: RandomForestClassifier, predict_df: pd.DataFrame, feature_columns: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Predict tips for rows with missing tip values.\n",
    "\n",
    "    Args:\n",
    "        model (RandomForestClassifier): Trained model.\n",
    "        predict_df (pd.DataFrame): DataFrame with NaN tips.\n",
    "        feature_columns (list[str]): List of feature column names.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with order_id and predicted tip.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If required columns are missing.\n",
    "    \"\"\"\n",
    "    logger.info(\"Predicting tips...\")\n",
    "    try:\n",
    "        if predict_df.empty:\n",
    "            logger.warning(\"No rows with missing tips to predict.\")\n",
    "            return pd.DataFrame(columns=['order_id', 'tip'])\n",
    "\n",
    "        missing_cols = [col for col in feature_columns if col not in predict_df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing feature columns in predict_df: {missing_cols}\")\n",
    "\n",
    "        X_predict = predict_df[feature_columns]\n",
    "        predict_df = predict_df.copy()\n",
    "        predict_df['tip'] = model.predict(X_predict).astype('int')\n",
    "        result_df = predict_df[['order_id', 'tip']].copy()\n",
    "\n",
    "        logger.info(f\"Predictions made: {result_df.shape}\")\n",
    "        return result_df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def merge_with_template(predictions: pd.DataFrame, template_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Merge predictions with the template CSV.\n",
    "\n",
    "    Args:\n",
    "        predictions (pd.DataFrame): DataFrame with order_id and tip.\n",
    "        template_path (str): Path to the template CSV.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Merged DataFrame with Unnamed: 0, order_id, tip.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If template file is missing.\n",
    "        ValueError: If merge fails due to missing columns.\n",
    "    \"\"\"\n",
    "    logger.info(\"Merging predictions with template...\")\n",
    "    try:\n",
    "        template = pd.read_csv(template_path)\n",
    "        if 'Unnamed: 0' not in template.columns or 'order_id' not in template.columns:\n",
    "            raise ValueError(\"Template missing required columns: 'Unnamed: 0', 'order_id'\")\n",
    "\n",
    "        merged_df = template[['Unnamed: 0', 'order_id']].merge(predictions, on='order_id', how='left')\n",
    "        logger.info(f\"Merged template: {merged_df.shape}\")\n",
    "        return merged_df\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Template file not found: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Merge failed: {e}\")\n",
    "        raise\n",
    "\n",
    "@task\n",
    "def save_predictions(predictions: pd.DataFrame, output_path: str) -> None:\n",
    "    \"\"\"Save the predictions to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        predictions (pd.DataFrame): DataFrame with predictions.\n",
    "        output_path (str): Path to save the CSV file.\n",
    "\n",
    "    Raises:\n",
    "        IOError: If saving fails.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Saving predictions to {output_path}...\")\n",
    "    try:\n",
    "        predictions.to_csv(output_path, index=False)\n",
    "        logger.info(f\"Predictions saved successfully to {output_path}\")\n",
    "    except IOError as e:\n",
    "        logger.error(f\"Failed to save predictions: {e}\")\n",
    "        raise\n",
    "\n",
    "@flow(name=\"Tip-Prediction-Pipeline\", log_prints=True)\n",
    "def tip_prediction_pipeline():\n",
    "    \"\"\"Orchestrate the tip prediction pipeline using Prefect.\n",
    "\n",
    "    The pipeline performs the following steps:\n",
    "    1. Load datasets.\n",
    "    2. Engineer features.\n",
    "    3. Save features to CSV.\n",
    "    4. Split data into training and prediction sets.\n",
    "    5. Train a Random Forest model.\n",
    "    6. Predict tips for NaN rows.\n",
    "    7. Merge predictions with template.\n",
    "    8. Save final predictions.\n",
    "    \"\"\"\n",
    "    # Define file paths\n",
    "    orders_path = \"orders.parquet\"\n",
    "    products_path = \"order_products_denormalized.csv\"\n",
    "    tips_path = \"tips_public.csv\"\n",
    "    features_output_path = \"all_features.csv\"\n",
    "    predictions_output_path = \"predicted_tips.csv\"\n",
    "    template_path = \"tip_testdaten_template_V2.csv\"\n",
    "\n",
    "    # Define feature columns\n",
    "    feature_columns = [\n",
    "        'order_has_alcohol', 'order_product_count', 'order_unique_dept_count',\n",
    "        'order_unique_aisle_count', 'order_unique_dept_ratio', 'order_unique_aisle_ratio',\n",
    "        'order_dept_tip_rate', 'order_aisle_tip_rate', 'order_placed_hour',\n",
    "        'order_placed_dow', 'order_is_weekend', 'order_placed_hour_sin',\n",
    "        'order_placed_hour_cos', 'order_placed_season_sin', 'order_placed_season_cos',\n",
    "        'order_time_since_last_hours', 'user_alcohol_purchase_count',\n",
    "        'user_total_purchase_count', 'user_unique_product_count',\n",
    "        'user_unique_to_total_ratio', 'user_frequent_purchase_hour',\n",
    "        'user_frequent_purchase_dow', 'user_avg_order_interval_hours',\n",
    "        'user_frequent_hour_sin', 'user_frequent_hour_cos',\n",
    "        'user_frequent_season_sin', 'user_frequent_season_cos',\n",
    "        'user_total_product_purchase_count', 'user_product_tip_prob'\n",
    "    ]\n",
    "\n",
    "    # Run pipeline\n",
    "    logger.info(\"Starting tip prediction pipeline...\")\n",
    "    orders, order_products_denormalized, tips_public = load_data(orders_path, products_path, tips_path)\n",
    "    all_features_df = engineer_features(orders, order_products_denormalized, tips_public)\n",
    "    save_features(all_features_df, features_output_path)\n",
    "    train_df, predict_df = split_data(all_features_df)\n",
    "    model, accuracy, report = train_model(train_df, feature_columns)\n",
    "    predictions = predict_tips(model, predict_df, feature_columns)\n",
    "    merged_predictions = merge_with_template(predictions, template_path)\n",
    "    save_predictions(merged_predictions, predictions_output_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tip_prediction_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_overview",
   "metadata": {},
   "source": [
    "## Feature Overview\n",
    "\n",
    "The following table lists all features engineered in this pipeline, aggregated to the `order_id` level.\n",
    "\n",
    "| **Feature Name** | **Level** | **Output Columns** | **Data Type** | **Description** |\n",
    "|------------------|-----------|--------------------|---------------|-----------------|\n",
    "| `user_alcohol_purchase_count` | User | `[user_id, user_alcohol_purchase_count]` | Integer | Counts the total number of alcohol products purchased by each user across all orders, merged via user_id. |\n",
    "| `user_total_purchase_count` | User | `[user_id, user_total_purchase_count]` | Integer | Counts the total number of products purchased by each user across all orders, merged via user_id. |\n",
    "| `user_unique_product_count` | User | `[user_id, user_unique_product_count]` | Integer | Counts the number of unique products purchased by each user, merged via user_id. |\n",
    "| `user_unique_to_total_ratio` | User | `[user_id, user_unique_to_total_ratio]` | Float | Calculates the ratio of unique products to total products purchased by each user, merged via user_id. |\n",
    "| `user_frequent_purchase_hour` | User | `[user_id, user_frequent_purchase_hour]` | Integer (0–23) | Identifies the hour of the day when the user places the most orders, defaulting to 12 (noon) if missing, merged via user_id. |\n",
    "| `user_frequent_purchase_dow` | User | `[user_id, user_frequent_purchase_dow]` | Integer (0–6) | Identifies the day of the week (0=Monday, 6=Sunday) when the user places the most orders, defaulting to 0 (Monday), merged via user_id. |\n",
    "| `user_avg_order_interval_hours` | User | `[user_id, user_avg_order_interval_hours]` | Float | Calculates the average time (in hours) between consecutive orders for each user, using the dataset median for users with one order, merged via user_id. |\n",
    "| `user_frequent_hour_sin`, `user_frequent_hour_cos` | User | `[user_id, user_frequent_hour_sin, user_frequent_hour_cos]` | Float (-1 to 1) | Applies sine-cosine transformation to the most frequent purchase hour to capture its cyclical nature, merged via user_id. |\n",
    "| `user_frequent_season_sin`, `user_frequent_season_cos` | User | `[user_id, user_frequent_season_sin, user_frequent_season_cos]` | Float (-1 to 1) | Applies sine-cosine transformation to the most frequent purchase month to capture seasonal cyclicality, defaulting to January, merged via user_id. |\n",
    "| `order_has_alcohol` | Order | `[order_id, order_has_alcohol]` | Integer (0 or 1) | Flags whether an order contains any alcohol products (1 if yes, 0 if no). |\n",
    "| `order_product_count` | Order | `[order_id, order_product_count]` | Integer | Counts the total number of items (products) in each order. |\n",
    "| `order_unique_dept_count` | Order | `[order_id, order_unique_dept_count]` | Integer | Counts the number of unique departments in each order. |\n",
    "| `order_unique_aisle_count` | Order | `[order_id, order_unique_aisle_count]` | Integer | Counts the number of unique aisles in each order. |\n",
    "| `order_unique_dept_ratio` | Order | `[order_id, order_unique_dept_ratio]` | Float | Calculates the ratio of unique departments to total items in each order. |\n",
    "| `order_unique_aisle_ratio` | Order | `[order_id, order_unique_aisle_ratio]` | Float | Calculates the ratio of unique aisles to total items in each order. |\n",
    "| `order_dept_tip_rate` | Order | `[order_id, order_dept_tip_rate]` | Float (0 to 1) | Computes the average tip rate for the departments in an order based on prior orders, defaulting to 0.500111 for no history. |\n",
    "| `order_aisle_tip_rate` | Order | `[order_id, order_aisle_tip_rate]` | Float (0 to 1) | Computes the average tip rate for the aisles in an order based on prior orders, defaulting to 0.500111 for no history. |\n",
    "| `order_placed_hour` | Order | `[order_id, order_placed_hour]` | Integer (0–23) | Extracts the hour of the day when the order was placed. |\n",
    "| `order_placed_dow` | Order | `[order_id, order_placed_dow]` | Integer (0–6) | Extracts the day of the week (0=Monday, 6=Sunday) when the order was placed. |\n",
    "| `order_is_weekend` | Order | `[order_id, order_is_weekend]` | Integer (0 or 1) | Flags whether the order was placed on a weekend (Saturday or Sunday). |\n",
    "| `order_placed_hour_sin`, `order_placed_hour_cos` | Order | `[order_id, order_placed_hour_sin, order_placed_hour_cos]` | Float (-1 to 1) | Applies sine-cosine transformation to the order’s hour to capture its cyclical nature. |\n",
    "| `order_placed_season_sin`, `order_placed_season_cos` | Order | `[order_id, order_placed_season_sin, order_placed_season_cos]` | Float (-1 to 1) | Applies sine-cosine transformation to the order’s month to capture seasonal cyclicality. |\n",
    "| `order_time_since_last_hours` | Order | `[order_id, order_time_since_last_hours]` | Float | Calculates the time (in hours) since the user’s previous order, using the dataset median for first orders. |\n",
    "| `user_total_product_purchase_count` | User | `[user_id, user_total_product_purchase_count]` | Integer | Total count of products purchased by each user, aggregated from user-product level, merged via user_id. |\n",
    "| `user_product_tip_prob` | Order | `[order_id, user_product_tip_prob]` | Float (0 to 1) | Average tip probability for user-product pairs in an order, aggregated to order_id, defaulting to 0.500111 for no history. |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
